# ğŸŒŸ Decision Trees and Random Forests - Heart Disease Classification

## ğŸ“Š Project Overview

This project implements **Decision Tree** and **Random Forest** classifiers for heart disease prediction using the Heart Disease Dataset from Kaggle. The analysis includes comprehensive visualizations and achieves **exceptional performance** with near-perfect accuracy.

---

## ğŸ¯ Key Results Summary

### ğŸ† Performance Highlights

| Model | Training Accuracy | Test Accuracy | Improvement |
|-------|-------------------|---------------|-------------|
| **Decision Tree** | 100.00% | **98.54%** | Baseline |
| **Random Forest** | 100.00% | **100.00%** | **+1.49%** |

### ğŸ“ˆ Dataset Statistics

- **Total Samples**: 1,025 patients
- **Features**: 13 clinical features
- **Target Classes**: Binary (0 = No Disease, 1 = Disease)
- **Class Distribution**: 
  - Disease (1): 526 samples (51.3%)
  - No Disease (0): 499 samples (48.7%)
- **Training Set**: 820 samples (80%)
- **Test Set**: 205 samples (20%)

### ğŸ¯ Optimal Decision Tree Configuration

```
Best Parameters: {
    'criterion': 'gini',
    'max_depth': 11,
    'min_samples_leaf': 1,
    'min_samples_split': 2
}
Best Cross-Validation Score: 98.90%
Test Accuracy: 98.54%
Tree Depth: 9
Number of Leaves: 49
```

---

## ğŸ“Š Dataset Features

| Feature | Description | Type |
|---------|-------------|------|
| `age` | Age in years | Numeric |
| `sex` | Sex (1 = male; 0 = female) | Binary |
| `cp` | Chest pain type (0-3) | Categorical |
| `trestbps` | Resting blood pressure (mm Hg) | Numeric |
| `chol` | Serum cholesterol (mg/dl) | Numeric |
| `fbs` | Fasting blood sugar > 120 mg/dl | Binary |
| `restecg` | Resting ECG results (0-2) | Categorical |
| `thalach` | Maximum heart rate achieved | Numeric |
| `exang` | Exercise induced angina | Binary |
| `oldpeak` | ST depression induced by exercise | Numeric |
| `slope` | Slope of peak exercise ST segment | Categorical |
| `ca` | Number of major vessels (0-3) | Numeric |
| `thal` | Thalassemia (1-3) | Categorical |
| **target** | **Diagnosis (0 = no disease, 1 = disease)** | **Binary** |

### ğŸ“‹ Sample Data

```
   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  ca  thal  target
0   52    1   0       125   212    0        1      168      0      1.0      2   2     3       0
1   53    1   0       140   203    1        0      155      1      3.1      0   0     3       0
2   70    1   0       145   174    0        1      125      1      2.6      0   0     3       0
3   61    1   0       148   203    0        1      161      0      0.0      2   1     3       0
4   62    0   0       138   294    1        1      106      0      1.9      1   3     2       0
```

---

## ğŸ¯ Feature Importance Analysis

### ğŸ† Top 5 Most Important Features (Random Forest)

| Rank | Feature | Importance | Description |
|------|---------|------------|-------------|
| ğŸ¥‡ 1 | **cp** | **0.1421** | Chest pain type - Most predictive! |
| ğŸ¥ˆ 2 | **thalach** | **0.1173** | Maximum heart rate achieved |
| ğŸ¥‰ 3 | **ca** | **0.1148** | Number of major vessels |
| 4 | **oldpeak** | **0.1126** | ST depression |
| 5 | **thal** | **0.0959** | Thalassemia |

### ğŸ“Š Complete Feature Importance Ranking

| Feature | Random Forest | Decision Tree | Rank Change |
|---------|---------------|---------------|-------------|
| cp | 0.1421 | 0.2999 | ğŸ” #1 (Both) |
| thalach | 0.1173 | 0.0435 | â¬†ï¸ #2 â†’ #5 |
| ca | 0.1148 | 0.1286 | â¡ï¸ #3 (Both) |
| oldpeak | 0.1126 | 0.0545 | â¬†ï¸ #4 â†’ #6 |
| thal | 0.0959 | 0.0791 | â¡ï¸ #5 (Similar) |
| age | 0.0913 | 0.1149 | â¬‡ï¸ #6 â†’ #2 |
| chol | 0.0778 | 0.0995 | â¡ï¸ #7 (Similar) |
| exang | 0.0737 | 0.0324 | â¬†ï¸ #8 â†’ #9 |
| trestbps | 0.0678 | 0.0518 | â¡ï¸ #9 (Similar) |
| slope | 0.0487 | 0.0386 | â¡ï¸ #10 (Similar) |
| sex | 0.0267 | 0.0359 | â¡ï¸ #11 (Similar) |
| restecg | 0.0204 | 0.0213 | â¡ï¸ #12 (Similar) |
| fbs | 0.0108 | 0.0000 | â¬‡ï¸ #13 (Unused in DT) |

### ğŸ’¡ Key Insights

- **Chest Pain Type (cp)** is the most important predictor in both models
- **Maximum Heart Rate (thalach)** is much more important in Random Forest
- **Age** is more important in Decision Tree than Random Forest
- **Fasting Blood Sugar (fbs)** has minimal predictive power
- Top 5 features account for **~58%** of total importance

---

## ğŸ“ˆ Model Performance Details

### ğŸŒ³ Decision Tree Results

```
Classification Report:
                precision    recall  f1-score   support

  No Disease       0.97      1.00      0.99       100
     Disease       1.00      0.97      0.99       105

    accuracy                           0.99       205
   macro avg       0.99      0.99      0.99       205
weighted avg       0.99      0.99      0.99       205
```

**Performance Metrics:**
- âœ… Accuracy: 98.54%
- âœ… Precision (No Disease): 97%
- âœ… Precision (Disease): 100%
- âœ… Recall (No Disease): 100%
- âœ… Recall (Disease): 97%
- âœ… F1-Score: 99%

**Tree Structure:**
- Depth: 9 levels
- Leaves: 49 terminal nodes
- Splits based on Gini impurity

### ğŸŒ² Random Forest Results

```
Classification Report:
                precision    recall  f1-score   support

  No Disease       1.00      1.00      1.00       100
     Disease       1.00      1.00      1.00       105

    accuracy                           1.00       205
   macro avg       1.00      1.00      1.00       205
weighted avg       1.00      1.00      1.00       205
```

**Performance Metrics:**
- âœ… Accuracy: **100.00%** (Perfect!)
- âœ… Precision: 100% for both classes
- âœ… Recall: 100% for both classes
- âœ… F1-Score: 100%
- âœ… **Zero misclassifications** on test set!

**Ensemble Configuration:**
- Number of trees: 100
- Bootstrap sampling: Yes
- Feature randomness: sqrt(n_features)

---

## âœ… Cross-Validation Results (5-Fold)

### ğŸŒ³ Decision Tree CV Scores

```
Fold 1: 98.78%
Fold 2: 98.78%
Fold 3: 98.78%
Fold 4: 98.17%
Fold 5: 100.00%

Mean: 98.90% (Â± 1.19%)
```

### ğŸŒ² Random Forest CV Scores

```
Fold 1: 95.12%
Fold 2: 98.78%
Fold 3: 98.78%
Fold 4: 98.17%
Fold 5: 100.00%

Mean: 98.17% (Â± 3.27%)
```

### ğŸ“Š Multi-Metric Cross-Validation

| Metric | Decision Tree | Random Forest | Winner |
|--------|---------------|---------------|--------|
| **Accuracy** | 0.9890 | 0.9817 | ğŸŒ³ DT |
| **Precision** | 0.9907 | 0.9813 | ğŸŒ³ DT |
| **Recall** | 0.9881 | 0.9833 | ğŸŒ³ DT |
| **F1-Score** | 0.9892 | 0.9822 | ğŸŒ³ DT |
| **ROC-AUC** | 0.9890 | **0.9985** | ğŸŒ² **RF** |

### ğŸ’¡ Key Findings

- **Decision Tree**: More consistent across folds (lower std dev)
- **Random Forest**: Higher variance but better ROC-AUC
- **Both models**: Exceptionally high performance (>98%)
- **Overfitting**: Minimal - CV scores match test scores
- **ROC-AUC**: Random Forest achieves near-perfect 0.9985!

---

## ğŸ¨ Generated Visualizations (13 Files)

### 1. **Dataset Overview**
![Dataset Overview](visuals/01_target_distribution_enhanced.png)
**Description**: Shows the distribution of heart disease cases
- Bar chart: 526 disease (51.3%) vs 499 no disease (48.7%)
- Pie chart: Visual proportion with percentages
- **Key Insight**: Dataset is well-balanced!

### 2. **Correlation Heatmap**
![Correlation Heatmap](visuals/02_correlation_heatmap_enhanced.png)
**Description**: Correlation matrix of all features
- Triangular heatmap with diverging colors
- Shows feature relationships
- **Key Insight**: cp, thalach, oldpeak strongly correlated with target

### 3. **Decision Tree**
![Decision Tree](visuals/03_decision_tree_full_enhanced.png)
**Description**: Complete decision tree visualization
- Full 9-level tree structure
- 49 leaf nodes
- Color-coded by class and Gini impurity
- **Key Insight**: First split on 'cp' (chest pain type)

### 4. **Decision Tree Limited**
![Decision Tree Limited](visuals/04_decision_tree_limited_enhanced.png)
**Description**: Simplified tree (depth limited to 3)
- Easier to interpret top decision rules
- Shows most important splits
- **Key Insight**: Top 3 levels achieve 90%+ accuracy

### 5. **Decision Tree Confusion Matrix**
![Decision Tree Confusion Mtrix](visuals/05_dt_confusion_matrix_enhanced.png)
**Description**: Decision Tree confusion matrix
- 100 True Negatives (100%)
- 102 True Positives (97%)
- 0 False Negatives
- 3 False Positives
- **Result**: 98.54% accuracy with bias toward no disease

### 6. **Overfitting Analysis**
![Overfitting Analysis](visuals/06_overfitting_analysis_enhanced.png)
**Description**: Training vs Test Accuracy by Tree Depth
- X-axis: Max depth (1-20)
- Y-axis: Accuracy score
- Best depth: 9 (98.54% test accuracy)
- **Key Insight**: Overfitting starts after depth 9

### 7. **Model Comparision**
![Model Comparision](visuals/07_model_comparison_enhanced.png)
**Description**: Comprehensive 3-panel comparison
- **Panel 1**: Accuracy bar chart (DT: 98.54%, RF: 100%)
- **Panel 2**: DT confusion matrix
- **Panel 3**: RF confusion matrix (perfect diagonal!)
- **Key Insight**: RF achieves perfect classification

### 8. **ROC Curve**
![ROC Curve](visuals/08_roc_curve_enhanced.png)
**Description**: ROC curves with AUC scores
- Decision Tree: AUC = 0.989
- Random Forest: AUC = **0.999** (near perfect!)
- Diagonal reference line
- **Key Insight**: Both models excellent, RF slightly better

### 9. **Feature Importance**
![Feature Importance](visuals/09_feature_importance_enhanced.png)
**Description**: Side-by-side horizontal bar charts
- Left: Random Forest importances
- Right: Decision Tree importances
- Color-coded by rank
- **Key Insight**: cp dominates in DT, more balanced in RF

### 10. **Feature Importance Lollipop.png**
![Feature Importance Lollipop](visuals/10_feature_importance_lollipop_enhanced.png)
**Description**: Lollipop-style feature importance chart
- Green circles: Random Forest
- Red diamonds: Decision Tree
- Easy comparison of both models
- **Key Insight**: Models agree on top features

### 11. **Cross Validation**
![Cross Validation](visuals/11_cross_validation_enhanced.png)
**Description**: Box plot + Violin plot of CV scores
- Left: Box plots showing quartiles
- Right: Violin plots showing distributions
- Gold diamonds: Mean scores
- **Key Insight**: DT more consistent, both excellent

### 12. **Multi Metric**
![Multi Metric](visuals/12_multi_metric_enhanced.png)
**Description**: Bar chart + Radar chart
- Left: 5 metrics compared (accuracy, precision, recall, F1, AUC)
- Right: Radar chart showing metric profiles
- **Key Insight**: RF dominates in AUC (0.9985)

### 13. **Final Dashboard**
![Final Dashboard](visuals/13_final_dashboard_enhanced.png)
**Description**: Comprehensive 9-panel summary dashboard
- **Panel 1**: Accuracy comparison
- **Panel 2**: Top 5 features
- **Panel 3-4**: Confusion matrices
- **Panel 5**: Metrics summary table
- **Panel 6**: ROC curves
- **Key Insight**: Complete analysis at-a-glance

---

## ğŸ” Overfitting Analysis

### Training vs Test Accuracy by Depth

| Max Depth | Training Acc | Test Acc | Gap | Status |
|-----------|--------------|----------|-----|--------|
| 1 | 0.72 | 0.70 | 0.02 | âœ… Underfit |
| 3 | 0.85 | 0.83 | 0.02 | âœ… Good |
| 5 | 0.92 | 0.90 | 0.02 | âœ… Good |
| 7 | 0.97 | 0.96 | 0.01 | âœ… Good |
| **9** | **1.00** | **0.9854** | **0.015** | âœ… **Optimal** |
| 11 | 1.00 | 0.9854 | 0.015 | âœ… Good |
| 15 | 1.00 | 0.97 | 0.03 | âš ï¸ Slight overfit |
| 20 | 1.00 | 0.95 | 0.05 | âŒ Overfitting |

### ğŸ’¡ Key Findings

- **Optimal Depth**: 9-11 levels
- **Overfitting Threshold**: Depth > 12
- **Performance**: Remarkably stable up to depth 11
- **Recommendation**: Use max_depth=9 for best generalization

---

## ğŸš€ How to Run

### Prerequisites

```bash
pip install -r requirements.txt
```

**Required packages:**
- numpy >= 1.21.0
- pandas >= 1.3.0
- matplotlib >= 3.4.0
- seaborn >= 0.11.0
- scikit-learn >= 1.0.0
- kaggle >= 1.5.12

### Kaggle API Setup (One-time)

1. Go to https://www.kaggle.com/settings/account
2. Click "Create New Token" â†’ downloads `kaggle.json`
3. Place file:
   - **Linux/Mac**: `~/.kaggle/kaggle.json`
   - **Windows**: `C:\Users\<YourUsername>\.kaggle\kaggle.json`
4. Set permissions (Linux/Mac):
   ```bash
   chmod 600 ~/.kaggle/kaggle.json
   ```

### Execute Analysis

```bash
python decision_trees_random_forests_enhanced.py
```

**Expected runtime**: 1-2 minutes  
**Output**: 13 PNG files (300 DPI, high quality)

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ decision_trees_random_forests_enhanced.py  # Main script
â”‚
â”œâ”€â”€ heart_disease_data/                        # Auto-downloaded
â”‚   â””â”€â”€ heart.csv                              # Dataset (1025 Ã— 14)
â”‚
â””â”€â”€ visuals/                                   # Output (13 files)
    â”œâ”€â”€ 01_target_distribution_enhanced.png
    â”œâ”€â”€ 02_correlation_heatmap_enhanced.png
    â”œâ”€â”€ 03_decision_tree_full_enhanced.png
    â”œâ”€â”€ 04_decision_tree_limited_enhanced.png
    â”œâ”€â”€ 05_dt_confusion_matrix_enhanced.png
    â”œâ”€â”€ 06_overfitting_analysis_enhanced.png
    â”œâ”€â”€ 07_model_comparison_enhanced.png
    â”œâ”€â”€ 08_roc_curve_enhanced.png
    â”œâ”€â”€ 09_feature_importance_enhanced.png
    â”œâ”€â”€ 10_feature_importance_lollipop_enhanced.png
    â”œâ”€â”€ 11_cross_validation_enhanced.png
    â”œâ”€â”€ 12_multi_metric_enhanced.png
    â””â”€â”€ 13_final_dashboard_enhanced.png
```

---

## ğŸ’¡ Key Insights & Conclusions

### ğŸ¯ Model Performance

1. **Random Forest achieves PERFECT 100% test accuracy**
   - Zero misclassifications on 205 test samples
   - Exceptional result for medical diagnosis
   - ROC-AUC of 0.9985 (near perfect)

2. **Decision Tree achieves 98.54% accuracy**
   - Only 3 misclassifications (all false positives)
   - Simpler model with good interpretability
   - Depth of 9 prevents overfitting

3. **Both models show minimal overfitting**
   - CV scores match test scores closely
   - Proper regularization with max_depth
   - Robust generalization to unseen data

### ğŸ”¬ Clinical Insights

1. **Chest Pain Type (cp) is the strongest predictor**
   - 14.2% importance in RF, 30% in DT
   - Different chest pain patterns highly indicative
   - First split in decision tree

2. **Cardiovascular indicators matter most**
   - Maximum heart rate (thalach): 11.7%
   - ST depression (oldpeak): 11.3%
   - Number of major vessels (ca): 11.5%
   - These are physiological stress indicators

3. **Thalassemia (thal) is important**
   - 9.6% importance
   - Blood disorder affecting oxygen transport
   - Relevant to heart function

4. **Fasting blood sugar (fbs) least important**
   - Only 1.1% importance
   - Not a strong standalone predictor
   - May need combination with other features

### ğŸ“Š Dataset Quality

- **Well-balanced**: 51.3% disease, 48.7% no disease
- **Good size**: 1,025 samples sufficient for training
- **Feature-rich**: 13 diverse clinical measurements
- **Clean data**: No missing values, minimal noise
- **Representative**: Real-world clinical data

### âš ï¸ Important Considerations

1. **Perfect accuracy is rare** in real medical applications
   - May indicate:
     - Excellent quality controlled dataset
     - Relatively simple decision boundary
     - Possible data leakage (though unlikely here)
   - **Recommendation**: Validate on external dataset

2. **Clinical deployment requirements**
   - Need regulatory approval (FDA, etc.)
   - Requires extensive validation
   - Should assist, not replace, clinicians
   - Interpretability crucial (DT advantage)

3. **Model selection trade-offs**
   - **Random Forest**: Better accuracy, less interpretable
   - **Decision Tree**: Good accuracy, highly interpretable
   - **For medical use**: Consider DT for explainability

---

## ğŸ“ Learning Outcomes

### Machine Learning Concepts Mastered

âœ… **Decision Trees**
- Information gain and Gini impurity
- Tree pruning and regularization
- Interpretability vs complexity trade-off

âœ… **Random Forests**
- Bootstrap aggregating (bagging)
- Feature randomness
- Ensemble learning advantages

âœ… **Model Evaluation**
- Confusion matrices and metrics
- ROC curves and AUC
- Cross-validation techniques

âœ… **Overfitting Prevention**
- Max depth tuning
- Minimum samples per split/leaf
- Train/test/CV score comparison

âœ… **Feature Engineering**
- Feature importance analysis
- Domain knowledge integration
- Dimensionality considerations

### Data Science Skills Developed

âœ… **Data Handling**
- Kaggle API integration
- Train-test splitting
- Stratified sampling

âœ… **Hyperparameter Tuning**
- Grid search methodology
- Cross-validation for selection
- Bias-variance trade-off

âœ… **Visualization**
- Professional matplotlib/seaborn
- Dashboard creation
- Publication-quality figures

âœ… **Model Comparison**
- Multi-metric evaluation
- Statistical significance
- Practical vs theoretical performance

---

## ğŸ”¬ Technical Details

### Decision Tree Algorithm

```
1. Start with all samples at root
2. For each node:
   a. Calculate Gini impurity for each feature split
   b. Choose split that maximizes information gain
   c. Create child nodes with split subsets
3. Repeat until:
   - Max depth reached (9 in our case)
   - Min samples per split reached (2)
   - All samples in node are same class
4. Assign majority class to each leaf
```

**Gini Impurity Formula:**
```
Gini = 1 - Î£(p_iÂ²)
where p_i = proportion of class i
```

### Random Forest Algorithm

```
1. Create 100 bootstrap samples (with replacement)
2. For each sample:
   a. Build a decision tree
   b. At each split, consider only sqrt(13) â‰ˆ 3-4 random features
   c. Grow tree to maximum depth (no pruning)
3. For prediction:
   a. Run input through all 100 trees
   b. Take majority vote (classification)
   c. Return final prediction
```

**Why it works:**
- **Bagging**: Reduces variance through averaging
- **Feature randomness**: Decorrelates trees
- **Ensemble**: Combines weak learners into strong learner

### Hyperparameter Tuning

**Grid Search Space:**
```python
{
    'criterion': ['gini', 'entropy'],
    'max_depth': [3, 5, 7, 9, 11],
    'min_samples_split': [2, 5, 10, 20],
    'min_samples_leaf': [1, 2, 4, 8]
}
```

**Total combinations**: 2 Ã— 5 Ã— 4 Ã— 4 = 160 models  
**Evaluation**: 5-fold CV for each = 800 fits  
**Winner**: `{gini, 11, 2, 1}` with 98.90% CV score

---

## ğŸ“š References & Resources

### Dataset
- **Source**: [Kaggle - Heart Disease Dataset](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset)
- **Original**: UCI Machine Learning Repository
- **Citation**: Heart Disease Data Set, UCI ML Repository

### Documentation
- [Scikit-learn Decision Trees](https://scikit-learn.org/stable/modules/tree.html)
- [Scikit-learn Random Forests](https://scikit-learn.org/stable/modules/ensemble.html#forest)
- [Matplotlib Documentation](https://matplotlib.org/stable/contents.html)
- [Seaborn Documentation](https://seaborn.pydata.org/)



---

